{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow-object-detection-training-colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erfany78/tensorflow/blob/main/tensorflow_object_detection_training_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uQCnYPVDrsgx"
      },
      "cell_type": "markdown",
      "source": [
        "# [How to train an object detection model easy for free](https://www.dlology.com/blog/how-to-train-an-object-detection-model-easy-for-free/) | DLology Blog"
      ]
    },
    {
      "metadata": {
        "id": "yhzxsJb3dpWq"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "gnNXNQCjdniL"
      },
      "cell_type": "code",
      "source": [
        "# If you forked the repository, you can replace the link.\n",
        "repo_url = 'https://github.com/erfany78/tensorflow'\n",
        "\n",
        "# Training batch size. Batch size 12 just fits in Colabe's Tesla K80 GPU memory.\n",
        "batch_size = 12\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "# Number of training steps.\n",
        "num_steps = 1000 # 200000\n",
        "\n",
        "# Number of evaluation steps.\n",
        "num_eval_steps = 50"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4V-XE6kbkc1"
      },
      "cell_type": "markdown",
      "source": [
        "## Clone the `object_detection_demo` repository or your fork."
      ]
    },
    {
      "metadata": {
        "id": "dxc3DmvLQF3z",
        "outputId": "4b131253-f8d6-4fac-db00-4a78a571ffc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "!git pull"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'tensorflow' already exists and is not an empty directory.\n",
            "/content/tensorflow\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "bI8__uNS8-ns"
      },
      "cell_type": "markdown",
      "source": [
        "## Install required packages"
      ]
    },
    {
      "metadata": {
        "id": "ecpHEnka8Kix",
        "outputId": "1398496f-6d6e-471b-8249-8c2ae0d69b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "\n",
        "!pip install -q pycocotools\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'models' already exists and is not an empty directory.\n",
            "/content/models/research\n",
            "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\n",
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "u-k7uGThXlny"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare `tfrecord` files\n",
        "\n",
        "Use the following scripts to generate the `tfrecord` files.\n",
        "```bash\n",
        "# Convert train folder annotation xml files to a single csv file,\n",
        "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
        "python xml_to_csv.py -i data/images/train -o data/annotations/train_labels.csv -l data/annotations\n",
        "\n",
        "# Convert test folder annotation xml files to a single csv.\n",
        "python xml_to_csv.py -i data/images/test -o data/annotations/test_labels.csv\n",
        "\n",
        "# Generate `train.record`\n",
        "python generate_tfrecord.py --csv_input=data/annotations/train_labels.csv --output_path=data/annotations/train.record --img_path=data/images/train --label_map data/annotations/label_map.pbtxt\n",
        "\n",
        "# Generate `test.record`\n",
        "python generate_tfrecord.py --csv_input=data/annotations/test_labels.csv --output_path=data/annotations/test.record --img_path=data/images/test --label_map data/annotations/label_map.pbtxt\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "ezGDABRXXhPP",
        "outputId": "3fe3214b-dd87-4763-c01a-99a682e3c561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%cd {repo_dir_path}\n",
        "\n",
        "# Convert train folder annotation xml files to a single csv file,\n",
        "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
        "!python xml_to_csv.py -i data/images/train -o data/annotations/train_labels.csv -l data/annotations\n",
        "\n",
        "# Convert test folder annotation xml files to a single csv.\n",
        "!python xml_to_csv.py -i data/images/test -o data/annotations/test_labels.csv\n",
        "\n",
        "# Generate `train.record`\n",
        "!python generate_tfrecord.py --csv_input=data/annotations/train_labels.csv --output_path=data/annotations/train.record --img_path=data/images/train --label_map data/annotations/label_map.pbtxt\n",
        "\n",
        "# Generate `test.record`\n",
        "!python generate_tfrecord.py --csv_input=data/annotations/test_labels.csv --output_path=data/annotations/test.record --img_path=data/images/test --label_map data/annotations/label_map.pbtxt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tensorflow\n",
            "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\n",
            "Successfully converted xml to csv.\n",
            "Generate `data/annotations/label_map.pbtxt`\n",
            "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\n",
            "Successfully converted xml to csv.\n",
            "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\n",
            "Traceback (most recent call last):\n",
            "  File \"generate_tfrecord.py\", line 134, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 312, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 258, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"generate_tfrecord.py\", line 125, in main\n",
            "    tf_example = create_tf_example(group, path, label_map)\n",
            "  File \"generate_tfrecord.py\", line 54, in create_tf_example\n",
            "    encoded_jpg = fid.read()\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/lib/io/file_io.py\", line 122, in read\n",
            "    self._preread_check()\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/lib/io/file_io.py\", line 84, in _preread_check\n",
            "    compat.as_bytes(self.__name), 1024 * 512)\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: /content/tensorflow/data/images/train/WIN_20220222_17_42_00_Pro; No such file or directory\n",
            "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\n",
            "Traceback (most recent call last):\n",
            "  File \"generate_tfrecord.py\", line 134, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 312, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 258, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"generate_tfrecord.py\", line 125, in main\n",
            "    tf_example = create_tf_example(group, path, label_map)\n",
            "  File \"generate_tfrecord.py\", line 54, in create_tf_example\n",
            "    encoded_jpg = fid.read()\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/lib/io/file_io.py\", line 122, in read\n",
            "    self._preread_check()\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/lib/io/file_io.py\", line 84, in _preread_check\n",
            "    compat.as_bytes(self.__name), 1024 * 512)\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: /content/tensorflow/data/images/test/WIN_20220222_18_01_45_Pro; No such file or directory\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "tgd-fzAIkZlV"
      },
      "cell_type": "code",
      "source": [
        "test_record_fname = '/content/tensorflow/data/annotations/test.record'\n",
        "train_record_fname = '/content/tensorflow/data/annotations/train.record'\n",
        "label_map_pbtxt_fname = '/content/tensorflow/data/annotations/label_map.pbtxt'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orDCj6ihgUMR",
        "outputId": "222dee06-d125-446b-f20d-763dca2522c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content/models/research\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "MODEL = 'faster_rcnn_inception_v2_coco_2018_01_28'\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "pGhvAObeiIix",
        "outputId": "fc26833b-04e6-4e73-de5d-2369c188b382",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/pretrained_model\n",
            "total 111M\n",
            "drwxr-xr-x  3 345018 5000 4.0K Feb  1  2018 .\n",
            "drwxr-xr-x 23 root   root 4.0K Feb 22 23:45 ..\n",
            "-rw-r--r--  1 345018 5000   77 Feb  1  2018 checkpoint\n",
            "-rw-r--r--  1 345018 5000  55M Feb  1  2018 frozen_inference_graph.pb\n",
            "-rw-r--r--  1 345018 5000  51M Feb  1  2018 model.ckpt.data-00000-of-00001\n",
            "-rw-r--r--  1 345018 5000  16K Feb  1  2018 model.ckpt.index\n",
            "-rw-r--r--  1 345018 5000 5.5M Feb  1  2018 model.ckpt.meta\n",
            "-rw-r--r--  1 345018 5000 3.2K Feb  1  2018 pipeline.config\n",
            "drwxr-xr-x  3 345018 5000 4.0K Feb  1  2018 saved_model\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "pLPcG1tFiV_Z"
      },
      "cell_type": "code",
      "source": [
        "filename = '/content/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHnxlfRznPP3",
        "outputId": "9949d5cb-5fee-497f-8e78-35d08ad51535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "fine_tune_checkpoint"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/models/research/pretrained_model/model.ckpt'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "MvwtHlLOeRJD"
      },
      "cell_type": "markdown",
      "source": [
        "## Configuring a Training Pipeline"
      ]
    },
    {
      "metadata": {
        "id": "fG1nCNpUXcRU"
      },
      "cell_type": "code",
      "source": [
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YjtCbLF2i0wI"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "filename = '/content/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config'\n",
        "with open(filename) as f:\n",
        "    s = f.read()\n",
        "with open(filename, 'w') as f:\n",
        "    s = re.sub('PATH_TO_BE_CONFIGURED/model.ckpt', fine_tune_checkpoint, s)\n",
        "    s = re.sub('PATH_TO_BE_CONFIGURED/pet_faces_train.record-\\?\\?\\?\\?\\?-of-00010',\n",
        "               train_record_fname, s)\n",
        "    s = re.sub(\n",
        "        'PATH_TO_BE_CONFIGURED/pet_faces_val.record-\\?\\?\\?\\?\\?-of-00010', test_record_fname, s)\n",
        "    s = re.sub('PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt',\n",
        "               label_map_pbtxt_fname, s)\n",
        "    \n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "    \n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    f.write(s)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GH0MEEanocn6",
        "outputId": "da0d0ed1-96fa-4cb4-eb59-5c50c63a134f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!cat {filename}"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Faster R-CNN with Inception v2, configured for Oxford-IIIT Pets Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  faster_rcnn {\n",
            "    num_classes: 4\n",
            "    image_resizer {\n",
            "      keep_aspect_ratio_resizer {\n",
            "        min_dimension: 600\n",
            "        max_dimension: 1024\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'faster_rcnn_inception_v2'\n",
            "      first_stage_features_stride: 16\n",
            "    }\n",
            "    first_stage_anchor_generator {\n",
            "      grid_anchor_generator {\n",
            "        scales: [0.25, 0.5, 1.0, 2.0]\n",
            "        aspect_ratios: [0.5, 1.0, 2.0]\n",
            "        height_stride: 16\n",
            "        width_stride: 16\n",
            "      }\n",
            "    }\n",
            "    first_stage_box_predictor_conv_hyperparams {\n",
            "      op: CONV\n",
            "      regularizer {\n",
            "        l2_regularizer {\n",
            "          weight: 0.0\n",
            "        }\n",
            "      }\n",
            "      initializer {\n",
            "        truncated_normal_initializer {\n",
            "          stddev: 0.01\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    first_stage_nms_score_threshold: 0.0\n",
            "    first_stage_nms_iou_threshold: 0.7\n",
            "    first_stage_max_proposals: 300\n",
            "    first_stage_localization_loss_weight: 2.0\n",
            "    first_stage_objectness_loss_weight: 1.0\n",
            "    initial_crop_size: 14\n",
            "    maxpool_kernel_size: 2\n",
            "    maxpool_stride: 2\n",
            "    second_stage_box_predictor {\n",
            "      mask_rcnn_box_predictor {\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 1.0\n",
            "        fc_hyperparams {\n",
            "          op: FC\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.0\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            variance_scaling_initializer {\n",
            "              factor: 1.0\n",
            "              uniform: true\n",
            "              mode: FAN_AVG\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    second_stage_post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 0.0\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 300\n",
            "      }\n",
            "      score_converter: SOFTMAX\n",
            "    }\n",
            "    second_stage_localization_loss_weight: 2.0\n",
            "    second_stage_classification_loss_weight: 1.0\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 12\n",
            "  optimizer {\n",
            "    momentum_optimizer: {\n",
            "      learning_rate: {\n",
            "        manual_step_learning_rate {\n",
            "          initial_learning_rate: 0.0002\n",
            "          schedule {\n",
            "            step: 900000\n",
            "            learning_rate: .00002\n",
            "          }\n",
            "          schedule {\n",
            "            step: 1200000\n",
            "            learning_rate: .000002\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "    }\n",
            "    use_moving_average: false\n",
            "  }\n",
            "  gradient_clipping_by_norm: 10.0\n",
            "  fine_tune_checkpoint: \"/content/models/research/pretrained_model/model.ckpt\"\n",
            "  from_detection_checkpoint: true\n",
            "  load_all_detection_checkpoint_vars: true\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 1000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/tensorflow/data/annotations/train.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/tensorflow/data/annotations/label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  metrics_set: \"coco_detection_metrics\"\n",
            "  num_examples: 1101\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/tensorflow/data/annotations/test.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/tensorflow/data/annotations/label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "f11w0uO3jFCB"
      },
      "cell_type": "code",
      "source": [
        "model_dir = 'training/'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "23TECXvNezIF"
      },
      "cell_type": "markdown",
      "source": [
        "## Run Tensorboard(Optional)"
      ]
    },
    {
      "metadata": {
        "id": "0H2PZs-mSCmO",
        "outputId": "8648e219-60c8-464b-a7a9-24073399b47b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-22 23:45:29--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 54.237.133.81, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.5’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  5.37MB/s    in 2.5s    \n",
            "\n",
            "2022-02-22 23:45:31 (5.37 MB/s) - ‘ngrok-stable-linux-amd64.zip.5’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "G8o6r1o5SC5M"
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = model_dir\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ge1OX7gcSC7S"
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5GSGxZNh8rp"
      },
      "cell_type": "markdown",
      "source": [
        "### Get Tensorboard link"
      ]
    },
    {
      "metadata": {
        "id": "rjhPT9iPSJ6T",
        "outputId": "af8e4997-fa98-4901-c312-945a57407513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://2c3d-34-70-149-234.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JDddx2rPfex9"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ]
    },
    {
      "metadata": {
        "id": "nC7_syR1SJ9F"
      },
      "cell_type": "code",
      "source": [
        "# Optionally remove content in output model directory to fresh start.\n",
        "!rm -rf {model_dir}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CjDHjhKQofT5",
        "outputId": "62fe701b-f7b2-4b0a-84e4-f9968a0687af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip install tf_slim\n",
        "#!pip install lvis\n",
        "#!pip install tensorflow.io\n",
        "#!pip install tf-models-official\n",
        "#!pip uninstall opencv-python\n",
        "#!pip install opencv-python\n",
        "!pip install pycocotools==2.0.0\n",
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={filename} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --num_eval_steps={num_eval_steps}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycocotools==2.0.0\n",
            "  Downloading pycocotools-2.0.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.0-cp37-cp37m-linux_x86_64.whl size=263883 sha256=f7b179e3a1fe209b155e252b6ddb7419e176cbeb45595f0aa5e20acf5413d684\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/50/dc/e1f07e9eb5678a0ee21bc091220f1f3806ba8e48ef3f2083cb\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.4\n",
            "    Uninstalling pycocotools-2.0.4:\n",
            "      Successfully uninstalled pycocotools-2.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-models-official 2.8.0 requires tensorflow~=2.8.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed pycocotools-2.0.0\n",
            "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\n",
            "Using TensorFlow backend.\n",
            "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
            "W0222 23:47:34.971552 140276883724160 model_lib.py:841] Forced number of epochs for all eval validations to be 1.\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 1000\n",
            "I0222 23:47:34.971848 140276883724160 config_util.py:552] Maybe overwriting train_steps: 1000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0222 23:47:34.972044 140276883724160 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "I0222 23:47:34.972221 140276883724160 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
            "I0222 23:47:34.972400 140276883724160 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
            "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "W0222 23:47:34.972601 140276883724160 model_lib.py:857] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu None\n",
            "I0222 23:47:34.972795 140276883724160 model_lib.py:894] create_estimator_and_inputs: use_tpu False, export_to_tpu None\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9441048f50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "I0222 23:47:34.973378 140276883724160 estimator.py:212] Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9441048f50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f9441047b00>) includes params argument, but params are not passed to Estimator.\n",
            "W0222 23:47:34.973690 140276883724160 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f9441047b00>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "I0222 23:47:34.974251 140276883724160 estimator_training.py:186] Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "I0222 23:47:34.974518 140276883724160 training.py:612] Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "I0222 23:47:34.974889 140276883724160 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0222 23:47:34.982369 140276883724160 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Reading unweighted datasets: ['/content/tensorflow/data/annotations/train.record']\n",
            "I0222 23:47:35.011418 140276883724160 dataset_builder.py:163] Reading unweighted datasets: ['/content/tensorflow/data/annotations/train.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['/content/tensorflow/data/annotations/train.record']\n",
            "I0222 23:47:35.012546 140276883724160 dataset_builder.py:80] Reading record datasets for input file: ['/content/tensorflow/data/annotations/train.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I0222 23:47:35.012709 140276883724160 dataset_builder.py:81] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0222 23:47:35.012958 140276883724160 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0222 23:47:35.019424 140276883724160 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0222 23:47:35.044079 140276883724160 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f944105ef90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0222 23:47:35.099975 140276883724160 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f944105ef90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f9440fec050> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0222 23:47:35.357046 140276883724160 ag_logging.py:146] Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f9440fec050> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:111: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0222 23:47:35.358857 140276883724160 deprecation.py:323] From /content/models/research/object_detection/inputs.py:111: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:97: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0222 23:47:35.373039 140276883724160 deprecation.py:323] From /content/models/research/object_detection/inputs.py:97: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:284: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0222 23:47:35.553792 140276883724160 deprecation.py:323] From /content/models/research/object_detection/inputs.py:284: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0222 23:47:36.271052 140276883724160 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:2802: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0222 23:47:36.535595 140276883724160 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:2802: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "I0222 23:47:38.280058 140276883724160 regularizers.py:99] Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "I0222 23:47:38.469679 140276883724160 regularizers.py:99] Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0222 23:47:38.470971 140276883724160 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/spatial_transform_ops.py:479: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "W0222 23:47:47.578635 140276883724160 deprecation.py:506] From /content/models/research/object_detection/utils/spatial_transform_ops.py:479: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0222 23:47:48.286463 140276883724160 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "I0222 23:47:48.289430 140276883724160 regularizers.py:99] Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "I0222 23:47:48.310811 140276883724160 regularizers.py:99] Scale of 0 disables regularizer.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:461: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "W0222 23:47:51.877011 140276883724160 deprecation.py:323] From /content/models/research/object_detection/core/losses.py:461: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0222 23:48:02.700513 140276883724160 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0222 23:48:02.702200 140276883724160 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0222 23:48:07.089763 140276883724160 monitored_session.py:240] Graph was finalized.\n",
            "2022-02-22 23:48:07.102931: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2022-02-22 23:48:07.103294: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55abad98ea80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-02-22 23:48:07.103334: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-02-22 23:48:07.108287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-02-22 23:48:07.248196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-22 23:48:07.249076: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55abad98e540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2022-02-22 23:48:07.249112: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2022-02-22 23:48:07.250434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-22 23:48:07.251139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-02-22 23:48:07.269306: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2022-02-22 23:48:07.465855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2022-02-22 23:48:07.467062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2022-02-22 23:48:07.467467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2022-02-22 23:48:07.671431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2022-02-22 23:48:07.690331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2022-02-22 23:48:08.062184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-02-22 23:48:08.062451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-22 23:48:08.063374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-22 23:48:08.064112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2022-02-22 23:48:08.067573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2022-02-22 23:48:08.069404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2022-02-22 23:48:08.069444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2022-02-22 23:48:08.069466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2022-02-22 23:48:08.070620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-22 23:48:08.071522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-02-22 23:48:08.072288: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2022-02-22 23:48:08.072352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0222 23:48:14.018259 140276883724160 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0222 23:48:14.542187 140276883724160 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into training/model.ckpt.\n",
            "I0222 23:48:29.814655 140276883724160 basic_session_run_hooks.py:606] Saving checkpoints for 0 into training/model.ckpt.\n",
            "2022-02-22 23:48:42.495000: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "KP-tUdtnRybs",
        "outputId": "9d671a90-c104-421d-b1cd-ebb675d00d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "!ls {model_dir}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n",
            "ls: cannot access 'training/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_1Nrqw3nqnCh"
      },
      "cell_type": "code",
      "source": [
        "# Legacy way of training(also works).\n",
        "# !python /content/models/research/object_detection/legacy/train.py --logtostderr --train_dir={model_dir} --pipeline_config_path={filename}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OmSESMetj1sa"
      },
      "cell_type": "markdown",
      "source": [
        "## Exporting a Trained Inference Graph\n",
        "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
      ]
    },
    {
      "metadata": {
        "id": "DHoP90pUyKSq",
        "outputId": "7581e1bb-034d-4c2a-e336-cac390e4b919",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "lst = os.listdir(model_dir)\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "print(last_model_path)\n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=/content/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config \\\n",
        "    --output_directory=fine_tuned_model \\\n",
        "    --trained_checkpoint_prefix={last_model_path}"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-5193abf1c67c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'model.ckpt-'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'.meta'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\d+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training/'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "usgBZvkz0nqD"
      },
      "cell_type": "code",
      "source": [
        "!ls fine_tuned_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p09AOThWkaQv"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the model `.pb` file"
      ]
    },
    {
      "metadata": {
        "id": "FIqnjbWYsuQw"
      },
      "cell_type": "markdown",
      "source": [
        "### Option1 : upload the `.pb` file to your Google Drive\n",
        "Then download it from your Google Drive to local file system.\n",
        "\n",
        "During this step, you will be prompted to enter the token."
      ]
    },
    {
      "metadata": {
        "id": "hAqyASIJqjae"
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import os\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "pb_fname = '/content/models/research/fine_tuned_model/frozen_inference_graph.pb'\n",
        "fname = os.path.basename(pb_fname)\n",
        "# Create & upload a text file.\n",
        "uploaded = drive.CreateFile({'title': fname})\n",
        "uploaded.SetContentFile(pb_fname)\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2FKFq8RXs6bs"
      },
      "cell_type": "markdown",
      "source": [
        "### Option2 :  Download the `.pb` file directly to your local file system\n",
        "This method may not be stable when downloading large files like the model `.pb` file. Try **option 1** instead if not working."
      ]
    },
    {
      "metadata": {
        "id": "-bP0iMMnnr77"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/models/research/fine_tuned_model/frozen_inference_graph.pb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MFyCeiBb9BbS"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the `label_map.pbtxt` file"
      ]
    },
    {
      "metadata": {
        "id": "K1TbL6Ox8q6Z"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(label_map_pbtxt_fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1AgBj1l0v_W"
      },
      "cell_type": "code",
      "source": [
        "# !tar cfz fine_tuned_model.tar.gz fine_tuned_model\n",
        "# from google.colab import files\n",
        "# files.download('fine_tuned_model.tar.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mz1gX19GlVW7"
      },
      "cell_type": "markdown",
      "source": [
        "## Run inference test\n",
        "Test with images in repository `object_detection_demo/test` directory."
      ]
    },
    {
      "metadata": {
        "id": "Pzj9A4e5mj5l"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = '/content/models/research/fine_tuned_model' + \\\n",
        "    '/frozen_inference_graph.pb'\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"test\")\n",
        "\n",
        "assert os.path.isfile(PATH_TO_CKPT)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.*\"))\n",
        "assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
        "print(TEST_IMAGE_PATHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CG5YUMdg1Po7"
      },
      "cell_type": "code",
      "source": [
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "    image = Image.open(image_path)\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    # Visualization of the results of a detection.\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=8)\n",
        "    plt.figure(figsize=IMAGE_SIZE)\n",
        "    plt.imshow(image_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GStNeHWPkTcN"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}